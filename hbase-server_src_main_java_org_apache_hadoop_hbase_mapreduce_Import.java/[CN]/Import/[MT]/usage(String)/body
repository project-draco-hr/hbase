{
  if (errorMsg != null && errorMsg.length() > 0) {
    System.err.println("ERROR: " + errorMsg);
  }
  System.err.println("Usage: Import [options] <tablename> <inputdir>");
  System.err.println("By default Import will load data directly into HBase. To instead generate");
  System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
  System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
  System.err.println(" To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use");
  System.err.println("  -D" + FILTER_CLASS_CONF_KEY + "=<name of filter class>");
  System.err.println("  -D" + FILTER_ARGS_CONF_KEY + "=<comma separated list of args for filter");
  System.err.println(" NOTE: The filter will be applied BEFORE doing key renames via the " + CF_RENAME_PROP + " property. Futher, filters will only use the"+ " Filter#filterRowKey(byte[] buffer, int offset, int length) method to identify "+ " whether the current row needs to be ignored completely for processing and "+ " Filter#filterKeyValue(KeyValue) method to determine if the KeyValue should be added;"+ " Filter.ReturnCode#INCLUDE and #INCLUDE_AND_NEXT_COL will be considered as including"+ " the KeyValue.");
  System.err.println("For performance consider the following options:\n" + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false\n"+ "  -D" + WAL_DURABILITY + "=<Used while writing data to hbase."+ " Allowed values are the supported durability values"+ " like SKIP_WAL/ASYNC_WAL/SYNC_WAL/...>");
}
