{
  if (reconstructionLog == null || !fs.exists(reconstructionLog)) {
    return;
  }
  FileStatus[] stats=fs.listStatus(reconstructionLog);
  if (stats == null || stats.length == 0) {
    LOG.warn("Passed reconstruction log " + reconstructionLog + " is zero-length");
    return;
  }
  long maxSeqIdInLog=-1;
  TreeMap<HStoreKey,byte[]> reconstructedCache=new TreeMap<HStoreKey,byte[]>();
  SequenceFile.Reader logReader=new SequenceFile.Reader(this.fs,reconstructionLog,this.conf);
  try {
    HLogKey key=new HLogKey();
    HLogEdit val=new HLogEdit();
    long skippedEdits=0;
    long editsCount=0;
    int reportInterval=this.conf.getInt("hbase.hstore.report.interval.edits",2000);
    while (logReader.next(key,val)) {
      maxSeqIdInLog=Math.max(maxSeqIdInLog,key.getLogSeqNum());
      if (key.getLogSeqNum() <= maxSeqID) {
        skippedEdits++;
        continue;
      }
      byte[] column=val.getColumn();
      if (Bytes.equals(column,HLog.METACOLUMN) || !Bytes.equals(key.getRegionName(),info.getRegionName()) || !HStoreKey.matchingFamily(family.getName(),column)) {
        continue;
      }
      HStoreKey k=new HStoreKey(key.getRow(),column,val.getTimestamp());
      reconstructedCache.put(k,val.getVal());
      editsCount++;
      if (reporter != null && (editsCount % reportInterval) == 0) {
        reporter.progress();
      }
    }
    if (LOG.isDebugEnabled()) {
      LOG.debug("Applied " + editsCount + ", skipped "+ skippedEdits+ " because sequence id <= "+ maxSeqID);
    }
  }
  finally {
    logReader.close();
  }
  if (reconstructedCache.size() > 0) {
    if (LOG.isDebugEnabled()) {
      LOG.debug("flushing reconstructionCache");
    }
    internalFlushCache(reconstructedCache,maxSeqIdInLog + 1);
  }
}
