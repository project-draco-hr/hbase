{
  long flushed=0;
  if (cache.size() == 0) {
    return flushed;
  }
synchronized (flushLock) {
    long now=System.currentTimeMillis();
    HStoreFile flushedFile=new HStoreFile(conf,fs,basedir,info.getEncodedName(),family.getFamilyName(),-1L,null);
    MapFile.Writer out=flushedFile.getWriter(this.fs,this.compression,this.bloomFilter);
    int entries=0;
    try {
      for (      Map.Entry<HStoreKey,byte[]> es : cache.entrySet()) {
        HStoreKey curkey=es.getKey();
        byte[] bytes=es.getValue();
        TextSequence f=HStoreKey.extractFamily(curkey.getColumn());
        if (f.equals(this.family.getFamilyName())) {
          if (ttl == HConstants.FOREVER || now < curkey.getTimestamp() + ttl) {
            entries++;
            out.append(curkey,new ImmutableBytesWritable(bytes));
            flushed+=HRegion.getEntrySize(curkey,bytes);
          }
 else {
            if (LOG.isDebugEnabled()) {
              LOG.debug("internalFlushCache: " + curkey + ": expired, skipped");
            }
          }
        }
      }
    }
  finally {
      out.close();
    }
    long newStoreSize=flushedFile.length();
    storeSize+=newStoreSize;
    flushedFile.writeInfo(fs,logCacheFlushId);
    if (bloomFilter != null) {
      flushBloomFilter();
    }
    updateReaders(logCacheFlushId,flushedFile);
    if (LOG.isDebugEnabled()) {
      LOG.debug("Added " + FSUtils.getPath(flushedFile.getMapFilePath()) + " with "+ entries+ " entries, sequence id "+ logCacheFlushId+ ", data size "+ StringUtils.humanReadableInt(flushed)+ ", file size "+ StringUtils.humanReadableInt(newStoreSize));
    }
  }
  return flushed;
}
