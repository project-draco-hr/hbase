{
synchronized (flushLock) {
    HStoreFile flushedFile=new HStoreFile(conf,fs,basedir,info.getEncodedName(),family.getFamilyName(),-1L,null);
    String name=flushedFile.toString();
    MapFile.Writer out=flushedFile.getWriter(this.fs,this.compression,this.bloomFilter);
    int entries=0;
    long cacheSize=0;
    try {
      for (      Map.Entry<HStoreKey,byte[]> es : cache.entrySet()) {
        HStoreKey curkey=es.getKey();
        byte[] bytes=es.getValue();
        TextSequence f=HStoreKey.extractFamily(curkey.getColumn());
        if (f.equals(this.family.getFamilyName())) {
          entries++;
          out.append(curkey,new ImmutableBytesWritable(bytes));
          cacheSize+=curkey.getSize() + (bytes != null ? bytes.length : 0);
        }
      }
    }
  finally {
      out.close();
    }
    long newStoreSize=flushedFile.length();
    storeSize+=newStoreSize;
    flushedFile.writeInfo(fs,logCacheFlushId);
    if (bloomFilter != null) {
      flushBloomFilter();
    }
    this.lock.writeLock().lock();
    try {
      Long flushid=Long.valueOf(logCacheFlushId);
      this.readers.put(flushid,flushedFile.getReader(this.fs,this.bloomFilter));
      this.storefiles.put(flushid,flushedFile);
      if (LOG.isDebugEnabled()) {
        LOG.debug("Added " + name + " with "+ entries+ " entries, sequence id "+ logCacheFlushId+ ", data size "+ StringUtils.humanReadableInt(cacheSize)+ ", file size "+ StringUtils.humanReadableInt(newStoreSize)+ " for "+ this.storeName);
      }
    }
  finally {
      this.lock.writeLock().unlock();
    }
  }
}
