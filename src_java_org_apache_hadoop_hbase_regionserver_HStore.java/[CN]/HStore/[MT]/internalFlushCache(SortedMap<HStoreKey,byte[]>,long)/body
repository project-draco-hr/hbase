{
  long flushed=0;
  if (cache.size() == 0) {
    return flushed;
  }
synchronized (flushLock) {
    HStoreFile flushedFile=new HStoreFile(conf,fs,basedir,info.getEncodedName(),family.getFamilyName(),-1L,null);
    MapFile.Writer out=flushedFile.getWriter(this.fs,this.compression,this.bloomFilter);
    int entries=0;
    try {
      for (      Map.Entry<HStoreKey,byte[]> es : cache.entrySet()) {
        HStoreKey curkey=es.getKey();
        byte[] bytes=es.getValue();
        TextSequence f=HStoreKey.extractFamily(curkey.getColumn());
        if (f.equals(this.family.getFamilyName())) {
          entries++;
          out.append(curkey,new ImmutableBytesWritable(bytes));
          flushed+=HRegion.getEntrySize(curkey,bytes);
        }
      }
    }
  finally {
      out.close();
    }
    long newStoreSize=flushedFile.length();
    storeSize+=newStoreSize;
    flushedFile.writeInfo(fs,logCacheFlushId);
    if (bloomFilter != null) {
      flushBloomFilter();
    }
    this.lock.writeLock().lock();
    try {
      Long flushid=Long.valueOf(logCacheFlushId);
      this.readers.put(flushid,flushedFile.getReader(this.fs,this.bloomFilter));
      this.storefiles.put(flushid,flushedFile);
      if (LOG.isDebugEnabled()) {
        LOG.debug("Added " + FSUtils.getPath(flushedFile.getMapFilePath()) + " with "+ entries+ " entries, sequence id "+ logCacheFlushId+ ", data size "+ StringUtils.humanReadableInt(flushed)+ ", file size "+ StringUtils.humanReadableInt(newStoreSize));
      }
    }
  finally {
      this.lock.writeLock().unlock();
    }
  }
  return flushed;
}
