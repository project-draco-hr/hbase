{
  boolean forceSplit=this.info.shouldSplit(false);
  boolean doMajorCompaction=majorCompaction;
synchronized (compactLock) {
    long maxId=-1;
    List<HStoreFile> filesToCompact=null;
synchronized (storefiles) {
      if (this.storefiles.size() <= 0) {
        return null;
      }
      filesToCompact=new ArrayList<HStoreFile>(this.storefiles.values());
      maxId=this.storefiles.lastKey().longValue();
    }
    if (!doMajorCompaction) {
      doMajorCompaction=isMajorCompaction(filesToCompact);
    }
    boolean references=hasReferences(filesToCompact);
    if (!doMajorCompaction && !references && filesToCompact.size() < compactionThreshold) {
      return checkSplit(forceSplit);
    }
    if (!fs.exists(compactionDir) && !fs.mkdirs(compactionDir)) {
      LOG.warn("Mkdir on " + compactionDir.toString() + " failed");
      return checkSplit(forceSplit);
    }
    int countOfFiles=filesToCompact.size();
    long totalSize=0;
    long[] fileSizes=new long[countOfFiles];
    long skipped=0;
    int point=0;
    for (int i=0; i < countOfFiles; i++) {
      HStoreFile file=filesToCompact.get(i);
      Path path=file.getMapFilePath();
      int len=0;
      for (      FileStatus fstatus : fs.listStatus(path)) {
        len+=fstatus.getLen();
      }
      fileSizes[i]=len;
      totalSize+=len;
    }
    if (!doMajorCompaction && !references) {
      for (point=0; point < countOfFiles - 1; point++) {
        if ((fileSizes[point] < fileSizes[point + 1] * 2) && (countOfFiles - point) <= maxFilesToCompact) {
          break;
        }
        skipped+=fileSizes[point];
      }
      filesToCompact=new ArrayList<HStoreFile>(filesToCompact.subList(point,countOfFiles));
      if (filesToCompact.size() <= 1) {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Skipped compaction of 1 file; compaction size of " + this.storeNameStr + ": "+ StringUtils.humanReadableInt(totalSize)+ "; Skipped "+ point+ " files, size: "+ skipped);
        }
        return checkSplit(forceSplit);
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Compaction size of " + this.storeNameStr + ": "+ StringUtils.humanReadableInt(totalSize)+ "; Skipped "+ point+ " file(s), size: "+ skipped);
      }
    }
    List<MapFile.Reader> rdrs=new ArrayList<MapFile.Reader>();
    int nrows=createReaders(rdrs,filesToCompact);
    HStoreFile compactedOutputFile=new HStoreFile(conf,fs,this.compactionDir,this.info,family.getName(),-1L,null);
    if (LOG.isDebugEnabled()) {
      LOG.debug("Started compaction of " + rdrs.size() + " file(s)"+ (references ? ", hasReferences=true," : " ")+ " into "+ FSUtils.getPath(compactedOutputFile.getMapFilePath()));
    }
    MapFile.Writer writer=compactedOutputFile.getWriter(this.fs,this.compression,this.family.isBloomfilter(),nrows);
    setIndexInterval(writer);
    try {
      compact(writer,rdrs,doMajorCompaction);
    }
  finally {
      writer.close();
    }
    compactedOutputFile.writeInfo(fs,maxId,doMajorCompaction);
    completeCompaction(filesToCompact,compactedOutputFile);
    if (LOG.isDebugEnabled()) {
      LOG.debug("Completed " + (doMajorCompaction ? "major" : "") + " compaction of "+ this.storeNameStr+ " store size is "+ StringUtils.humanReadableInt(storeSize));
    }
  }
  return checkSplit(forceSplit);
}
