{
synchronized (compactLock) {
    long maxId=-1;
    int nrows=-1;
    List<HStoreFile> filesToCompact=null;
synchronized (storefiles) {
      if (this.storefiles.size() <= 0) {
        return null;
      }
      filesToCompact=new ArrayList<HStoreFile>(this.storefiles.values());
      maxId=this.storefiles.lastKey().longValue();
    }
    if (!force && !hasReferences(filesToCompact) && filesToCompact.size() < compactionThreshold) {
      return checkSplit();
    }
    if (!fs.exists(compactionDir) && !fs.mkdirs(compactionDir)) {
      LOG.warn("Mkdir on " + compactionDir.toString() + " failed");
      return checkSplit();
    }
    int countOfFiles=filesToCompact.size();
    long totalSize=0;
    long[] fileSizes=new long[countOfFiles];
    long skipped=0;
    int point=0;
    for (int i=0; i < countOfFiles; i++) {
      HStoreFile file=filesToCompact.get(i);
      Path path=file.getMapFilePath();
      int len=0;
      for (      FileStatus fstatus : fs.listStatus(path)) {
        len+=fstatus.getLen();
      }
      fileSizes[i]=len;
      totalSize+=len;
    }
    if (!force && !hasReferences(filesToCompact)) {
      for (point=0; point < compactionThreshold - 1; point++) {
        if (fileSizes[point] < fileSizes[point + 1] * 2) {
          break;
        }
        skipped+=fileSizes[point];
      }
      filesToCompact=new ArrayList<HStoreFile>(filesToCompact.subList(point,countOfFiles));
      if (LOG.isDebugEnabled()) {
        LOG.debug("Compaction size of " + this.storeNameStr + ": "+ StringUtils.humanReadableInt(totalSize)+ ", skipped "+ point+ ", "+ skipped);
      }
    }
    List<MapFile.Reader> readers=new ArrayList<MapFile.Reader>();
    for (    HStoreFile file : filesToCompact) {
      try {
        HStoreFile.BloomFilterMapFile.Reader reader=file.getReader(fs,false,false);
        readers.add(reader);
        if (this.family.isBloomfilter()) {
          nrows+=reader.getBloomFilterSize();
        }
      }
 catch (      IOException e) {
        LOG.warn("Failed with " + e.toString() + ": "+ file.toString());
        closeCompactionReaders(readers);
        throw e;
      }
    }
    Collections.reverse(readers);
    HStoreFile compactedOutputFile=new HStoreFile(conf,fs,this.compactionDir,info.getEncodedName(),family.getName(),-1L,null);
    if (LOG.isDebugEnabled()) {
      LOG.debug("started compaction of " + readers.size() + " files into "+ FSUtils.getPath(compactedOutputFile.getMapFilePath()));
    }
    MapFile.Writer writer=compactedOutputFile.getWriter(this.fs,this.compression,this.family.isBloomfilter(),nrows);
    writer.setIndexInterval(family.getMapFileIndexInterval());
    try {
      compactHStoreFiles(writer,readers);
    }
  finally {
      writer.close();
    }
    compactedOutputFile.writeInfo(fs,maxId);
    completeCompaction(filesToCompact,compactedOutputFile);
    if (LOG.isDebugEnabled()) {
      LOG.debug("Completed compaction of " + this.storeNameStr + " store size is "+ StringUtils.humanReadableInt(storeSize));
    }
  }
  return checkSplit();
}
