{
  boolean forceSplit=this.info.shouldSplit(false);
  boolean doMajorCompaction=majorCompaction;
synchronized (compactLock) {
    long maxId=-1;
    List<HStoreFile> filesToCompact=null;
synchronized (storefiles) {
      if (this.storefiles.size() <= 0) {
        return null;
      }
      filesToCompact=new ArrayList<HStoreFile>(this.storefiles.values());
      maxId=this.storefiles.lastKey().longValue();
    }
    long lastMajorCompaction=0L;
    if (!doMajorCompaction) {
      doMajorCompaction=isMajorCompaction();
    }
    if (!doMajorCompaction && !hasReferences(filesToCompact) && filesToCompact.size() < compactionThreshold) {
      return checkSplit(forceSplit);
    }
    if (!fs.exists(compactionDir) && !fs.mkdirs(compactionDir)) {
      LOG.warn("Mkdir on " + compactionDir.toString() + " failed");
      return checkSplit(forceSplit);
    }
    int countOfFiles=filesToCompact.size();
    long totalSize=0;
    long[] fileSizes=new long[countOfFiles];
    long skipped=0;
    int point=0;
    for (int i=0; i < countOfFiles; i++) {
      HStoreFile file=filesToCompact.get(i);
      Path path=file.getMapFilePath();
      int len=0;
      for (      FileStatus fstatus : fs.listStatus(path)) {
        len+=fstatus.getLen();
      }
      fileSizes[i]=len;
      totalSize+=len;
    }
    if (!doMajorCompaction && !hasReferences(filesToCompact)) {
      for (point=0; point < countOfFiles - 1; point++) {
        if ((fileSizes[point] < fileSizes[point + 1] * 2) && (countOfFiles - point) <= maxFilesToCompact) {
          break;
        }
        skipped+=fileSizes[point];
      }
      filesToCompact=new ArrayList<HStoreFile>(filesToCompact.subList(point,countOfFiles));
      if (filesToCompact.size() <= 1) {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Skipped compaction of 1 file; compaction size of " + this.storeNameStr + ": "+ StringUtils.humanReadableInt(totalSize)+ "; Skipped "+ point+ " files, size: "+ skipped);
        }
        return checkSplit(forceSplit);
      }
      if (LOG.isDebugEnabled()) {
        LOG.debug("Compaction size of " + this.storeNameStr + ": "+ StringUtils.humanReadableInt(totalSize)+ "; Skipped "+ point+ " files , size: "+ skipped);
      }
    }
    List<MapFile.Reader> rdrs=new ArrayList<MapFile.Reader>();
    int nrows=createReaders(rdrs,filesToCompact);
    HStoreFile compactedOutputFile=new HStoreFile(conf,fs,this.compactionDir,this.info,family.getName(),-1L,null);
    if (LOG.isDebugEnabled()) {
      LOG.debug("started compaction of " + rdrs.size() + " files into "+ FSUtils.getPath(compactedOutputFile.getMapFilePath()));
    }
    MapFile.Writer writer=compactedOutputFile.getWriter(this.fs,this.compression,this.family.isBloomfilter(),nrows);
    writer.setIndexInterval(family.getMapFileIndexInterval());
    try {
      compact(writer,rdrs,doMajorCompaction);
    }
  finally {
      writer.close();
    }
    compactedOutputFile.writeInfo(fs,maxId);
    completeCompaction(filesToCompact,compactedOutputFile);
    if (LOG.isDebugEnabled()) {
      LOG.debug("Completed compaction of " + this.storeNameStr + " store size is "+ StringUtils.humanReadableInt(storeSize)+ (doMajorCompaction ? "" : "; time since last major compaction: " + (lastMajorCompaction / 1000) + " seconds"));
    }
  }
  return checkSplit(forceSplit);
}
