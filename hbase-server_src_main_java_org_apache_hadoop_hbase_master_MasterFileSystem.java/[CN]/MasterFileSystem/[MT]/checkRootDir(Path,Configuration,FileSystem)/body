{
  FSUtils.waitOnSafeMode(c,c.getInt(HConstants.THREAD_WAKE_FREQUENCY,10 * 1000));
  try {
    if (!fs.exists(rd)) {
      fs.mkdirs(rd);
      FSUtils.setVersion(fs,rd,c.getInt(HConstants.THREAD_WAKE_FREQUENCY,10 * 1000),c.getInt(HConstants.VERSION_FILE_WRITE_ATTEMPTS,HConstants.DEFAULT_VERSION_FILE_WRITE_ATTEMPTS));
    }
 else {
      if (!fs.isDirectory(rd)) {
        throw new IllegalArgumentException(rd.toString() + " is not a directory");
      }
      FSUtils.checkVersion(fs,rd,true,c.getInt(HConstants.THREAD_WAKE_FREQUENCY,10 * 1000),c.getInt(HConstants.VERSION_FILE_WRITE_ATTEMPTS,HConstants.DEFAULT_VERSION_FILE_WRITE_ATTEMPTS));
    }
  }
 catch (  DeserializationException de) {
    LOG.fatal("Please fix invalid configuration for " + HConstants.HBASE_DIR,de);
    IOException ioe=new IOException();
    ioe.initCause(de);
    throw ioe;
  }
catch (  IllegalArgumentException iae) {
    LOG.fatal("Please fix invalid configuration for " + HConstants.HBASE_DIR + " "+ rd.toString(),iae);
    throw iae;
  }
  if (!FSUtils.checkClusterIdExists(fs,rd,c.getInt(HConstants.THREAD_WAKE_FREQUENCY,10 * 1000))) {
    FSUtils.setClusterId(fs,rd,new ClusterId(),c.getInt(HConstants.THREAD_WAKE_FREQUENCY,10 * 1000));
  }
  clusterId=FSUtils.getClusterId(fs,rd);
  if (!FSUtils.metaRegionExists(fs,rd)) {
    bootstrap(rd,c);
  }
 else {
    org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir.migrateFSTableDescriptorsIfNecessary(fs,rd);
  }
  FSTableDescriptors fsd=new FSTableDescriptors(c,fs,rd);
  fsd.createTableDescriptor(new TableDescriptor(fsd.get(TableName.META_TABLE_NAME)));
  return rd;
}
