{
  final HConnection conn=table.getConnection();
  if (!conn.isTableAvailable(table.getName())) {
    throw new TableNotFoundException("Table " + Bytes.toStringBinary(table.getTableName()) + "is not currently available.");
  }
  int nrThreads=getConf().getInt("hbase.loadincremental.threads.max",Runtime.getRuntime().availableProcessors());
  ThreadFactoryBuilder builder=new ThreadFactoryBuilder();
  builder.setNameFormat("LoadIncrementalHFiles-%1$d");
  ExecutorService pool=new ThreadPoolExecutor(nrThreads,nrThreads,60,TimeUnit.SECONDS,new LinkedBlockingQueue<Runnable>(),builder.build());
  ((ThreadPoolExecutor)pool).allowCoreThreadTimeOut(true);
  Deque<LoadQueueItem> queue=new LinkedList<LoadQueueItem>();
  try {
    discoverLoadQueue(queue,hfofDir);
    Collection<HColumnDescriptor> families=table.getTableDescriptor().getFamilies();
    ArrayList<String> familyNames=new ArrayList<String>();
    for (    HColumnDescriptor family : families) {
      familyNames.add(family.getNameAsString());
    }
    ArrayList<String> unmatchedFamilies=new ArrayList<String>();
    for (    LoadQueueItem lqi : queue) {
      String familyNameInHFile=Bytes.toString(lqi.family);
      if (!familyNames.contains(familyNameInHFile)) {
        unmatchedFamilies.add(familyNameInHFile);
      }
    }
    if (unmatchedFamilies.size() > 0) {
      String msg="Unmatched family names found: unmatched family names in HFiles to be bulkloaded: " + unmatchedFamilies + "; valid family names of table "+ Bytes.toString(table.getTableName())+ " are: "+ familyNames;
      LOG.error(msg);
      throw new IOException(msg);
    }
    int count=0;
    if (queue.isEmpty()) {
      LOG.warn("Bulk load operation did not find any files to load in " + "directory " + hfofDir.toUri() + ".  Does it contain files in "+ "subdirectories that correspond to column family names?");
      return;
    }
    if (userProvider.isHBaseSecurityEnabled()) {
      FileSystem fs=FileSystem.get(getConf());
      fsDelegationToken.acquireDelegationToken(fs);
      bulkToken=new SecureBulkLoadClient(table).prepareBulkLoad(table.getName());
    }
    while (!queue.isEmpty()) {
      final Pair<byte[][],byte[][]> startEndKeys=table.getStartEndKeys();
      if (count != 0) {
        LOG.info("Split occured while grouping HFiles, retry attempt " + +count + " with "+ queue.size()+ " files remaining to group or split");
      }
      int maxRetries=getConf().getInt("hbase.bulkload.retries.number",0);
      if (maxRetries != 0 && count >= maxRetries) {
        LOG.error("Retry attempted " + count + " times without completing, bailing out");
        return;
      }
      count++;
      Multimap<ByteBuffer,LoadQueueItem> regionGroups=groupOrSplitPhase(table,pool,queue,startEndKeys);
      if (!checkHFilesCountPerRegionPerFamily(regionGroups)) {
        throw new IOException("Trying to load more than " + maxFilesPerRegionPerFamily + " hfiles to one family of one region");
      }
      bulkLoadPhase(table,conn,pool,queue,regionGroups);
    }
  }
  finally {
    if (userProvider.isHBaseSecurityEnabled()) {
      fsDelegationToken.releaseDelegationToken();
      if (bulkToken != null) {
        new SecureBulkLoadClient(table).cleanupBulkLoad(bulkToken);
      }
    }
    pool.shutdown();
    if (queue != null && !queue.isEmpty()) {
      StringBuilder err=new StringBuilder();
      err.append("-------------------------------------------------\n");
      err.append("Bulk load aborted with some files not yet loaded:\n");
      err.append("-------------------------------------------------\n");
      for (      LoadQueueItem q : queue) {
        err.append("  ").append(q.hfilePath).append('\n');
      }
      LOG.error(err);
    }
  }
  if (queue != null && !queue.isEmpty()) {
    throw new RuntimeException("Bulk load aborted with some files not yet loaded." + "Please check log for more details.");
  }
}
