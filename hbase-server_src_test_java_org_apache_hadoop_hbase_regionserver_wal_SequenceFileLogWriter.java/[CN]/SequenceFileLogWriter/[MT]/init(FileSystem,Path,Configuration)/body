{
  boolean compress=conf.getBoolean(HConstants.ENABLE_WAL_COMPRESSION,false);
  if (compress) {
    try {
      if (this.compressionContext == null) {
        this.compressionContext=new CompressionContext(LRUDictionary.class);
      }
 else {
        this.compressionContext.clear();
      }
    }
 catch (    Exception e) {
      throw new IOException("Failed to initiate CompressionContext",e);
    }
  }
  try {
    this.writer=(SequenceFile.Writer)SequenceFile.class.getMethod("createWriter",new Class[]{FileSystem.class,Configuration.class,Path.class,Class.class,Class.class,Integer.TYPE,Short.TYPE,Long.TYPE,Boolean.TYPE,CompressionType.class,CompressionCodec.class,Metadata.class}).invoke(null,new Object[]{fs,conf,path,HLogKey.class,WALEdit.class,Integer.valueOf(FSUtils.getDefaultBufferSize(fs)),Short.valueOf((short)conf.getInt("hbase.regionserver.hlog.replication",FSUtils.getDefaultReplication(fs,path))),Long.valueOf(conf.getLong("hbase.regionserver.hlog.blocksize",FSUtils.getDefaultBlockSize(fs,path))),Boolean.valueOf(false),SequenceFile.CompressionType.NONE,new DefaultCodec(),createMetadata(conf,compress)});
  }
 catch (  InvocationTargetException ite) {
    throw new IOException(ite.getCause());
  }
catch (  Exception e) {
  }
  if (this.writer == null) {
    LOG.debug("new createWriter -- HADOOP-6840 -- not available");
    this.writer=SequenceFile.createWriter(fs,conf,path,HLogKey.class,WALEdit.class,FSUtils.getDefaultBufferSize(fs),(short)conf.getInt("hbase.regionserver.hlog.replication",FSUtils.getDefaultReplication(fs,path)),conf.getLong("hbase.regionserver.hlog.blocksize",FSUtils.getDefaultBlockSize(fs,path)),SequenceFile.CompressionType.NONE,new DefaultCodec(),null,createMetadata(conf,compress));
  }
 else {
    if (LOG.isTraceEnabled())     LOG.trace("Using new createWriter -- HADOOP-6840");
  }
  this.writer_out=getSequenceFilePrivateFSDataOutputStreamAccessible();
  if (LOG.isTraceEnabled())   LOG.trace("Path=" + path + ", compression="+ compress);
}
