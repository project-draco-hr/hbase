{
  Job job=null;
  try {
    LOG.info("Before map/reduce startup");
    job=new Job(table.getConfiguration(),"process column contents");
    job.setNumReduceTasks(1);
    Scan scan=new Scan();
    scan.addFamily(INPUT_FAMILY);
    TableMapReduceUtil.initTableMapperJob(table.getName().getNameAsString(),scan,ProcessContentsMapper.class,ImmutableBytesWritable.class,Put.class,job);
    TableMapReduceUtil.initTableReducerJob(table.getName().getNameAsString(),IdentityTableReducer.class,job);
    FileOutputFormat.setOutputPath(job,new Path("test"));
    LOG.info("Started " + table.getName().getNameAsString());
    assertTrue(job.waitForCompletion(true));
    LOG.info("After map/reduce completion");
    verify(table.getName());
  }
 catch (  InterruptedException e) {
    throw new IOException(e);
  }
catch (  ClassNotFoundException e) {
    throw new IOException(e);
  }
 finally {
    table.close();
    if (job != null) {
      FileUtil.fullyDelete(new File(job.getConfiguration().get("hadoop.tmp.dir")));
    }
  }
}
