{
  Job job=null;
  try {
    LOG.info("Before map/reduce startup");
    job=new Job(table.getConfiguration(),"process column contents");
    job.setNumReduceTasks(1);
    Scan scan=new Scan();
    scan.addFamily(INPUT_FAMILY);
    TableMapReduceUtil.initTableMapperJob(Bytes.toString(table.getTableName()),scan,ProcessContentsMapper.class,ImmutableBytesWritable.class,Put.class,job);
    TableMapReduceUtil.initTableReducerJob(Bytes.toString(table.getTableName()),IdentityTableReducer.class,job);
    FileOutputFormat.setOutputPath(job,new Path("test"));
    LOG.info("Started " + Bytes.toString(table.getTableName()));
    assertTrue(job.waitForCompletion(true));
    LOG.info("After map/reduce completion");
    verify(Bytes.toString(table.getTableName()));
  }
  finally {
    table.close();
    if (job != null) {
      FileUtil.fullyDelete(new File(job.getConfiguration().get("hadoop.tmp.dir")));
    }
  }
}
