{
  this.split=split;
  HTableDescriptor htd=split.htd;
  HRegionInfo hri=this.split.getRegionInfo();
  FileSystem fs=FSUtils.getCurrentFileSystem(conf);
  Path tmpRootDir=new Path(conf.get(RESTORE_DIR_KEY));
  if (conf.get(TableInputFormat.SCAN) != null) {
    scan=TableMapReduceUtil.convertStringToScan(conf.get(TableInputFormat.SCAN));
  }
 else   if (conf.get(org.apache.hadoop.hbase.mapred.TableInputFormat.COLUMN_LIST) != null) {
    String[] columns=conf.get(org.apache.hadoop.hbase.mapred.TableInputFormat.COLUMN_LIST).split(" ");
    scan=new Scan();
    for (    String col : columns) {
      scan.addFamily(Bytes.toBytes(col));
    }
  }
 else {
    throw new IllegalArgumentException("A Scan is not configured for this job");
  }
  scan.setIsolationLevel(IsolationLevel.READ_UNCOMMITTED);
  scan.setCacheBlocks(false);
  scanner=new ClientSideRegionScanner(conf,fs,tmpRootDir,htd,hri,scan,null);
}
