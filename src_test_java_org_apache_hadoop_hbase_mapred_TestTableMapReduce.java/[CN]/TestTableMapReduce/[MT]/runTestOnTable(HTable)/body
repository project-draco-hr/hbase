{
  JobConf jobConf=null;
  try {
    LOG.info("Before map/reduce startup");
    jobConf=new JobConf(UTIL.getConfiguration(),TestTableMapReduce.class);
    jobConf.setJobName("process column contents");
    jobConf.setNumReduceTasks(1);
    TableMapReduceUtil.initTableMapJob(Bytes.toString(table.getTableName()),Bytes.toString(INPUT_FAMILY),ProcessContentsMapper.class,ImmutableBytesWritable.class,Put.class,jobConf);
    TableMapReduceUtil.initTableReduceJob(Bytes.toString(table.getTableName()),IdentityTableReduce.class,jobConf);
    LOG.info("Started " + Bytes.toString(table.getTableName()));
    RunningJob job=JobClient.runJob(jobConf);
    assertTrue(job.isSuccessful());
    LOG.info("After map/reduce completion");
    verify(Bytes.toString(table.getTableName()));
  }
  finally {
    if (jobConf != null) {
      FileUtil.fullyDelete(new File(jobConf.get("hadoop.tmp.dir")));
    }
  }
}
