{
  Class<? extends SplitAlgorithm> splitClass=conf.getClass("split.algorithm",MD5StringSplit.class,SplitAlgorithm.class);
  SplitAlgorithm splitAlgo;
  try {
    splitAlgo=splitClass.newInstance();
  }
 catch (  Exception e) {
    throw new IOException("Problem loading split algorithm: ",e);
  }
  final int minOS=conf.getInt("split.outstanding",2);
  HTable table=new HTable(conf,tableName);
  final int MAX_OUTSTANDING=Math.max(table.getCurrentNrHRS() / 2,minOS);
  Path hbDir=new Path(conf.get(HConstants.HBASE_DIR));
  Path tableDir=HTableDescriptor.getTableDir(hbDir,table.getTableName());
  Path splitFile=new Path(tableDir,"_balancedSplit");
  FileSystem fs=FileSystem.get(conf);
  LinkedList<Pair<byte[],byte[]>> tmpRegionSet=getSplits(table,splitAlgo);
  LinkedList<Pair<byte[],byte[]>> outstanding=Lists.newLinkedList();
  int splitCount=0;
  final int origCount=tmpRegionSet.size();
  LOG.debug("Bucketing regions by regionserver...");
  TreeMap<HServerAddress,LinkedList<Pair<byte[],byte[]>>> daughterRegions=Maps.newTreeMap();
  for (  Pair<byte[],byte[]> dr : tmpRegionSet) {
    HServerAddress rsLocation=table.getRegionLocation(dr.getSecond()).getServerAddress();
    if (!daughterRegions.containsKey(rsLocation)) {
      LinkedList<Pair<byte[],byte[]>> entry=Lists.newLinkedList();
      daughterRegions.put(rsLocation,entry);
    }
    daughterRegions.get(rsLocation).add(dr);
  }
  LOG.debug("Done with bucketing.  Split time!");
  long startTime=System.currentTimeMillis();
  FSDataInputStream tmpIn=fs.open(splitFile);
  byte[] rawData=new byte[tmpIn.available()];
  tmpIn.readFully(rawData);
  tmpIn.close();
  FSDataOutputStream splitOut=fs.create(splitFile);
  splitOut.write(rawData);
  try {
    while (!daughterRegions.isEmpty()) {
      LOG.debug(daughterRegions.size() + " RS have regions to splt.");
      for (HServerAddress rsLoc=daughterRegions.firstKey(); rsLoc != null; rsLoc=daughterRegions.higherKey(rsLoc)) {
        Pair<byte[],byte[]> dr=null;
        LOG.debug("Finding a region on " + rsLoc);
        LinkedList<Pair<byte[],byte[]>> regionList=daughterRegions.get(rsLoc);
        while (!regionList.isEmpty()) {
          dr=regionList.pop();
          byte[] split=dr.getSecond();
          HRegionLocation regionLoc=table.getRegionLocation(split);
          HServerAddress newRs=regionLoc.getServerAddress();
          if (newRs.compareTo(rsLoc) != 0) {
            LOG.debug("Region with " + splitAlgo.rowToStr(split) + " moved to "+ newRs+ ". Relocating...");
            if (!daughterRegions.containsKey(newRs)) {
              LinkedList<Pair<byte[],byte[]>> entry=Lists.newLinkedList();
              daughterRegions.put(newRs,entry);
            }
            daughterRegions.get(newRs).add(dr);
            dr=null;
            continue;
          }
          byte[] sk=regionLoc.getRegionInfo().getStartKey();
          if (sk.length != 0) {
            if (Bytes.equals(split,sk)) {
              LOG.debug("Region already split on " + splitAlgo.rowToStr(split) + ".  Skipping this region...");
              dr=null;
              continue;
            }
            byte[] start=dr.getFirst();
            Preconditions.checkArgument(Bytes.equals(start,sk),splitAlgo.rowToStr(start) + " != " + splitAlgo.rowToStr(sk));
          }
          break;
        }
        if (regionList.isEmpty()) {
          daughterRegions.remove(rsLoc);
        }
        if (dr == null)         continue;
        byte[] start=dr.getFirst();
        byte[] split=dr.getSecond();
        LOG.debug("Splitting at " + splitAlgo.rowToStr(split));
        HBaseAdmin admin=new HBaseAdmin(table.getConfiguration());
        admin.split(table.getTableName(),split);
        if (conf.getBoolean("split.verify",true)) {
          boolean daughterOnline=false;
          int daughterSleep=5;
          while (!daughterOnline) {
            LOG.debug("Waiting for daughter region to come online...");
            table.clearRegionCache();
            HRegionInfo hri=table.getRegionLocation(split).getRegionInfo();
            daughterOnline=Bytes.equals(hri.getStartKey(),split) && !hri.isOffline();
            daughterSleep=Math.min(daughterSleep * 2,60);
            Thread.sleep(daughterSleep * 1000);
          }
          LOG.debug("Daughter region is online.");
        }
        splitOut.writeChars("- " + splitAlgo.rowToStr(dr.getFirst()) + " "+ splitAlgo.rowToStr(dr.getSecond())+ "\n");
        splitCount++;
        if (splitCount % 10 == 0) {
          long tDiff=(System.currentTimeMillis() - startTime) / splitCount;
          LOG.debug("STATUS UPDATE: " + splitCount + " / "+ origCount+ ". Avg Time / Split = "+ org.apache.hadoop.util.StringUtils.formatTime(tDiff));
        }
        if (conf.getBoolean("split.verify",true)) {
          outstanding.addLast(Pair.newPair(start,split));
          if (outstanding.size() > MAX_OUTSTANDING) {
            Pair<byte[],byte[]> reg=outstanding.removeFirst();
            String outStart=splitAlgo.rowToStr(reg.getFirst());
            String outSplit=splitAlgo.rowToStr(reg.getSecond());
            LOG.debug("Waiting for " + outStart + " , "+ outSplit+ " to finish compaction");
            LinkedList<HRegionInfo> check=Lists.newLinkedList();
            check.add(table.getRegionLocation(reg.getFirst()).getRegionInfo());
            check.add(table.getRegionLocation(reg.getSecond()).getRegionInfo());
            while (!check.isEmpty()) {
              for (              HRegionInfo hri : check.toArray(new HRegionInfo[]{})) {
                boolean refFound=false;
                byte[] sk=hri.getStartKey();
                if (sk.length == 0)                 sk=splitAlgo.firstRow();
                String startKey=splitAlgo.rowToStr(sk);
                for (                HColumnDescriptor c : hri.getTableDesc().getFamilies()) {
                  Path cfDir=Store.getStoreHomedir(tableDir,hri.getEncodedName(),c.getName());
                  if (fs.exists(cfDir)) {
                    for (                    FileStatus file : fs.listStatus(cfDir)) {
                      refFound|=StoreFile.isReference(file.getPath());
                      if (refFound) {
                        LOG.debug("Reference still exists for " + startKey + " at "+ file.getPath());
                        break;
                      }
                    }
                  }
                  if (refFound)                   break;
                }
                if (!refFound) {
                  check.remove(hri);
                  LOG.debug("- finished compaction of " + startKey);
                }
              }
              if (!check.isEmpty()) {
                LOG.debug("Waiting for " + check.size() + " compactions");
                Thread.sleep(30 * 1000);
              }
            }
          }
        }
      }
    }
    LOG.debug("All regions have been sucesfully split!");
  }
  finally {
    long tDiff=System.currentTimeMillis() - startTime;
    LOG.debug("TOTAL TIME = " + org.apache.hadoop.util.StringUtils.formatTime(tDiff));
    LOG.debug("Splits = " + splitCount);
    LOG.debug("Avg Time / Split = " + org.apache.hadoop.util.StringUtils.formatTime(tDiff / splitCount));
    splitOut.close();
  }
  fs.delete(splitFile,false);
}
