{
  boolean compress=conf.getBoolean(HConstants.ENABLE_WAL_COMPRESSION,false);
  if (compress) {
    try {
      if (this.compressionContext == null) {
        this.compressionContext=new CompressionContext(LRUDictionary.class);
      }
 else {
        this.compressionContext.clear();
      }
    }
 catch (    Exception e) {
      throw new IOException("Failed to initiate CompressionContext",e);
    }
  }
  if (null == keyClass) {
    keyClass=HLog.getKeyClass(conf);
  }
  try {
    this.writer=(SequenceFile.Writer)SequenceFile.class.getMethod("createWriter",new Class[]{FileSystem.class,Configuration.class,Path.class,Class.class,Class.class,Integer.TYPE,Short.TYPE,Long.TYPE,Boolean.TYPE,CompressionType.class,CompressionCodec.class,Metadata.class}).invoke(null,new Object[]{fs,conf,path,HLog.getKeyClass(conf),WALEdit.class,Integer.valueOf(fs.getConf().getInt("io.file.buffer.size",4096)),Short.valueOf((short)conf.getInt("hbase.regionserver.hlog.replication",fs.getDefaultReplication())),Long.valueOf(conf.getLong("hbase.regionserver.hlog.blocksize",fs.getDefaultBlockSize())),Boolean.valueOf(false),SequenceFile.CompressionType.NONE,new DefaultCodec(),createMetadata(conf,compress)});
  }
 catch (  InvocationTargetException ite) {
    throw new IOException(ite.getCause());
  }
catch (  Exception e) {
  }
  if (this.writer == null) {
    LOG.debug("new createWriter -- HADOOP-6840 -- not available");
    this.writer=SequenceFile.createWriter(fs,conf,path,HLog.getKeyClass(conf),WALEdit.class,fs.getConf().getInt("io.file.buffer.size",4096),(short)conf.getInt("hbase.regionserver.hlog.replication",fs.getDefaultReplication()),conf.getLong("hbase.regionserver.hlog.blocksize",fs.getDefaultBlockSize()),SequenceFile.CompressionType.NONE,new DefaultCodec(),null,createMetadata(conf,compress));
  }
 else {
    LOG.debug("using new createWriter -- HADOOP-6840");
  }
  this.writer_out=getSequenceFilePrivateFSDataOutputStreamAccessible();
  LOG.debug("Path=" + path + ", compression="+ compress);
}
