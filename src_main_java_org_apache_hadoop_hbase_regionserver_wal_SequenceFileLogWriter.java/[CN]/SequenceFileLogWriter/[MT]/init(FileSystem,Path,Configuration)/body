{
  if (null == keyClass) {
    keyClass=HLog.getKeyClass(conf);
  }
  try {
    this.writer=(SequenceFile.Writer)SequenceFile.class.getMethod("createWriter",new Class[]{FileSystem.class,Configuration.class,Path.class,Class.class,Class.class,Integer.TYPE,Short.TYPE,Long.TYPE,Boolean.TYPE,CompressionType.class,CompressionCodec.class,Metadata.class}).invoke(null,new Object[]{fs,conf,path,HLog.getKeyClass(conf),WALEdit.class,Integer.valueOf(fs.getConf().getInt("io.file.buffer.size",4096)),Short.valueOf((short)conf.getInt("hbase.regionserver.hlog.replication",fs.getDefaultReplication())),Long.valueOf(conf.getLong("hbase.regionserver.hlog.blocksize",fs.getDefaultBlockSize())),Boolean.valueOf(false),SequenceFile.CompressionType.NONE,new DefaultCodec(),new Metadata()});
  }
 catch (  InvocationTargetException ite) {
    throw new IOException(ite.getCause());
  }
catch (  Exception e) {
  }
  if (this.writer == null) {
    LOG.debug("new createWriter -- HADOOP-6840 -- not available");
    this.writer=SequenceFile.createWriter(fs,conf,path,HLog.getKeyClass(conf),WALEdit.class,fs.getConf().getInt("io.file.buffer.size",4096),(short)conf.getInt("hbase.regionserver.hlog.replication",fs.getDefaultReplication()),conf.getLong("hbase.regionserver.hlog.blocksize",fs.getDefaultBlockSize()),SequenceFile.CompressionType.NONE,new DefaultCodec(),null,new Metadata());
  }
 else {
    LOG.debug("using new createWriter -- HADOOP-6840");
  }
  this.writer_out=getSequenceFilePrivateFSDataOutputStreamAccessible();
  this.syncFs=getSyncFs();
  this.hflush=getHFlush();
  String msg="Path=" + path + ", syncFs="+ (this.syncFs != null)+ ", hflush="+ (this.hflush != null);
  if (this.syncFs != null || this.hflush != null) {
    LOG.debug(msg);
  }
 else {
    LOG.warn("No sync support! " + msg);
  }
}
