{
  Job job=null;
  try {
    LOG.info("Before map/reduce startup");
    job=new Job(table.getConfiguration(),"process column contents");
    job.setNumReduceTasks(1);
    Scan scan=new Scan();
    scan.addFamily(INPUT_FAMILY);
    TableMapReduceUtil.initTableMapperJob(table.getName(),scan,MultithreadedTableMapper.class,ImmutableBytesWritable.class,Put.class,job);
    MultithreadedTableMapper.setMapperClass(job,ProcessContentsMapper.class);
    MultithreadedTableMapper.setNumberOfThreads(job,NUMBER_OF_THREADS);
    TableMapReduceUtil.initTableReducerJob(table.getName().getNameAsString(),IdentityTableReducer.class,job);
    FileOutputFormat.setOutputPath(job,new Path("test"));
    LOG.info("Started " + table.getName());
    assertTrue(job.waitForCompletion(true));
    LOG.info("After map/reduce completion");
    verify(table.getName());
  }
  finally {
    table.close();
    if (job != null) {
      FileUtil.fullyDelete(new File(job.getConfiguration().get("hadoop.tmp.dir")));
    }
  }
}
