{
  if (this.writer != null && this.numEntries.get() <= 0) {
    return null;
  }
  byte[][] regionsToFlush=null;
  this.cacheFlushLock.lock();
  try {
    if (closed) {
      return regionsToFlush;
    }
    long currentFilenum=this.filenum;
    this.filenum=System.currentTimeMillis();
    Path newPath=computeFilename();
    HLog.Writer nextWriter=createWriter(fs,newPath,HBaseConfiguration.create(conf));
    int nextInitialReplication=fs.getFileStatus(newPath).getReplication();
    OutputStream nextHdfsOut=null;
    if (nextWriter instanceof SequenceFileLogWriter) {
      nextHdfsOut=((SequenceFileLogWriter)nextWriter).getDFSCOutputStream();
    }
synchronized (updateLock) {
      Path oldFile=cleanupCurrentWriter(currentFilenum);
      this.writer=nextWriter;
      this.initialReplication=nextInitialReplication;
      this.hdfs_out=nextHdfsOut;
      LOG.info((oldFile != null ? "Roll " + FSUtils.getPath(oldFile) + ", entries="+ this.numEntries.get()+ ", filesize="+ this.fs.getFileStatus(oldFile).getLen()+ ". " : "") + "New hlog " + FSUtils.getPath(newPath));
      this.numEntries.set(0);
    }
    if (!this.actionListeners.isEmpty()) {
      for (      LogActionsListener list : this.actionListeners) {
        list.logRolled(newPath);
      }
    }
    if (this.outputfiles.size() > 0) {
      if (this.lastSeqWritten.size() <= 0) {
        LOG.debug("Last sequenceid written is empty. Deleting all old hlogs");
        for (        Map.Entry<Long,Path> e : this.outputfiles.entrySet()) {
          archiveLogFile(e.getValue(),e.getKey());
        }
        this.outputfiles.clear();
      }
 else {
        regionsToFlush=cleanOldLogs();
      }
    }
  }
  finally {
    this.cacheFlushLock.unlock();
  }
  return regionsToFlush;
}
