import argparse
import logging
import re
import requests
parser = argparse.ArgumentParser()
parser.add_argument('--max-builds', type=int, metavar='n', help=('Number of builds to analyze for each job (if available in jenkins). Default: all ' + 'available builds.'))
parser.add_argument('--mvn', action='store_true', help=(('Writes two strings for including/excluding these flaky tests using maven flags. These ' + 'strings are written to files so they can be saved as artifacts and easily imported in ') + 'other projects.'))
parser.add_argument('-v', '--verbose', help='Prints more logs.', action='store_true')
parser.add_argument('urls', help='Space separated list of urls (single/multi-configuration project) to analyze')
args = parser.parse_args()
logging.basicConfig()
logger = logging.getLogger('org.apache.hadoop.hbase.report-flakies')
if args.verbose:
    logger.setLevel(logging.INFO)
jobs_list = []
for url in args.urls.split():
    json_response = requests.get((url + '/api/json')).json()
    if json_response.has_key('activeConfigurations'):
        for config in json_response['activeConfigurations']:
            jobs_list.append(config['url'])
    elif json_response.has_key('builds'):
        jobs_list.append(url)
    else:
        raise Exception('Bad url ({0}).'.format(url))
global_bad_tests = set()
for job_url in jobs_list:
    logger.info('Analyzing job: %s', job_url)
    build_id_to_results = {}
    builds = requests.get((job_url + '/api/json')).json()['builds']
    num_builds = 0
    build_ids = []
    build_ids_without_result = []
    for build in builds:
        build_id = build['number']
        build_ids.append(build_id)
        build_result = get_build_results(build['url'])
        if (len(build_result) > 0):
            build_id_to_results[build_id] = build_result
        else:
            build_ids_without_result.append(build_id)
        num_builds += 1
        if (num_builds == args.max_builds):
            break
    bad_tests = set()
    for build in build_id_to_results:
        for test in build_id_to_results[build]:
            if ((build_id_to_results[build][test] == 'REGRESSION') or (build_id_to_results[build][test] == 'FAILED')):
                bad_tests.add(test)
                global_bad_tests.add(test)
    build_counts = {key: dict([('total', 0), ('failed', 0)]) for key in bad_tests}
    for build in build_id_to_results:
        build_results = build_id_to_results[build]
        for bad_test in bad_tests:
            if build_results.has_key(bad_test):
                if (build_results[bad_test] != 'SKIPPED'):
                    build_counts[bad_test]['total'] += 1
                if (build_results[bad_test] == 'REGRESSION'):
                    build_counts[bad_test]['failed'] += 1
    if (len(bad_tests) > 0):
        print 'Job: {}'.format(job_url)
        print '{:>100}  {:6}  {:10}  {}'.format('Test Name', 'Failed', 'Total Runs', 'Flakyness')
        for bad_test in bad_tests:
            fail = build_counts[bad_test]['failed']
            total = build_counts[bad_test]['total']
            print '{:>100}  {:6}  {:10}  {:2.0f}%'.format(bad_test, fail, total, ((fail * 100.0) / total))
    else:
        print 'No flaky tests founds.'
        if (len(build_ids) == len(build_ids_without_result)):
            print 'None of the analyzed builds have test result.'
    print ('Builds analyzed: ' + str(build_ids))
    print ('Builds with no results: ' + str(build_ids_without_result))
    print ''
if args.mvn:
    test_cases = set()
    for test in global_bad_tests:
        test = re.sub('.*\\.', '', test)
        test = re.sub('#.*', '', test)
        test_cases.add(test)
    includes = ','.join(test_cases)
    with open('./includes', 'w') as inc_file:
        inc_file.write(includes)
        inc_file.close()
    excludes = ''
    for test_case in test_cases:
        excludes += (('**/' + test_case) + '.java,')
    with open('./excludes', 'w') as exc_file:
        exc_file.write(excludes)
        exc_file.close()
