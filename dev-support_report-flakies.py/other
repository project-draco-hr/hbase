'\nThis script uses Jenkins REST api to collect test result(s) of given build/builds and generates\nflakyness data about unittests.\nPrint help: report-flakies.py -h\n'
import argparse
import logging
import os
import time
from collections import OrderedDict
from jinja2 import Template
import requests
import findHangingTests
parser = argparse.ArgumentParser()
parser.add_argument('--urls', metavar='URL', action='append', required=True, help='Urls to analyze, which can refer to simple projects, multi-configuration projects or individual build run.')
parser.add_argument('--excluded-builds', metavar='n1,n2', action='append', help='List of build numbers to exclude (or "None"). Not required, but if specified, number of uses should be same as that of --urls since the values are matched.')
parser.add_argument('--max-builds', metavar='n', action='append', type=int, help='The maximum number of builds to use (if available on jenkins). Specify 0 to analyze all builds. Not required, but if specified, number of uses should be same as that of --urls since the values are matched.')
parser.add_argument('--mvn', action='store_true', help='Writes two strings for including/excluding these flaky tests using maven flags. These strings are written to files so they can be saved as artifacts and easily imported in other projects. Also writes timeout and failing tests in separate files for reference.')
parser.add_argument('-v', '--verbose', help='Prints more logs.', action='store_true')
args = parser.parse_args()
logging.basicConfig()
logger = logging.getLogger(__name__)
if args.verbose:
    logger.setLevel(logging.INFO)
all_timeout_tests = set()
all_failed_tests = set()
all_hanging_tests = set()
url_to_bad_test_results = OrderedDict()
expanded_urls = expand_multi_config_projects(args)
for url_max_build in expanded_urls:
    url = url_max_build['url']
    excludes = url_max_build['excludes']
    json_response = requests.get((url + '/api/json')).json()
    if json_response.has_key('builds'):
        builds = json_response['builds']
        logger.info('Analyzing job: %s', url)
    else:
        builds = [{'number': json_response['id'], 'url': url, }]
        logger.info('Analyzing build : %s', url)
    build_id_to_results = {}
    num_builds = 0
    build_ids = []
    build_ids_without_tests_run = []
    for build in builds:
        build_id = build['number']
        if (build_id in excludes):
            continue
        result = get_bad_tests(build['url'])
        if (not result):
            continue
        if (len(result[0]) > 0):
            build_id_to_results[build_id] = result
        else:
            build_ids_without_tests_run.append(build_id)
        num_builds += 1
        build_ids.append(build_id)
        if (num_builds == url_max_build['max_builds']):
            break
    bad_tests = set()
    for build in build_id_to_results:
        [_, failed_tests, timeout_tests, hanging_tests] = build_id_to_results[build]
        all_timeout_tests.update(timeout_tests)
        all_failed_tests.update(failed_tests)
        all_hanging_tests.update(hanging_tests)
        bad_tests.update(failed_tests.union(hanging_tests))
    test_to_build_ids = {key: {'all': set(), 'timeout': set(), 'failed': set(), 'hanging': set(), 'bad_count': 0, } for key in bad_tests}
    for build in build_id_to_results:
        [all_tests, failed_tests, timeout_tests, hanging_tests] = build_id_to_results[build]
        for bad_test in test_to_build_ids:
            is_bad = False
            if all_tests.issuperset([bad_test]):
                test_to_build_ids[bad_test]['all'].add(build)
            if timeout_tests.issuperset([bad_test]):
                test_to_build_ids[bad_test]['timeout'].add(build)
                is_bad = True
            if failed_tests.issuperset([bad_test]):
                test_to_build_ids[bad_test]['failed'].add(build)
                is_bad = True
            if hanging_tests.issuperset([bad_test]):
                test_to_build_ids[bad_test]['hanging'].add(build)
                is_bad = True
            if is_bad:
                test_to_build_ids[bad_test]['bad_count'] += 1
    for bad_test in test_to_build_ids:
        test_to_build_ids[bad_test]['flakyness'] = ((test_to_build_ids[bad_test]['bad_count'] * 100.0) / len(test_to_build_ids[bad_test]['all']))
    sorted_test_to_build_ids = OrderedDict(sorted(test_to_build_ids.iteritems(), key=(lambda x: x[1]['flakyness']), reverse=True))
    url_to_bad_test_results[url] = sorted_test_to_build_ids
    if (len(sorted_test_to_build_ids) > 0):
        print 'URL: {}'.format(url)
        print '{:>60}  {:10}  {:25}  {}'.format('Test Name', 'Total Runs', 'Bad Runs(failed/timeout/hanging)', 'Flakyness')
        for bad_test in sorted_test_to_build_ids:
            test_status = sorted_test_to_build_ids[bad_test]
            print '{:>60}  {:10}  {:7} ( {:4} / {:5} / {:5} )  {:2.0f}%'.format(bad_test, len(test_status['all']), test_status['bad_count'], len(test_status['failed']), len(test_status['timeout']), len(test_status['hanging']), test_status['flakyness'])
    else:
        print 'No flaky tests founds.'
        if (len(build_ids) == len(build_ids_without_tests_run)):
            print 'None of the analyzed builds have test result.'
    print 'Builds analyzed: {}'.format(build_ids)
    print 'Builds without any test runs: {}'.format(build_ids_without_tests_run)
    print ''
all_bad_tests = all_hanging_tests.union(all_failed_tests)
if args.mvn:
    includes = ','.join(all_bad_tests)
    with open('./includes', 'w') as inc_file:
        inc_file.write(includes)
    excludes = ['**/{0}.java'.format(bad_test) for bad_test in all_bad_tests]
    with open('./excludes', 'w') as exc_file:
        exc_file.write(','.join(excludes))
    with open('./timeout', 'w') as timeout_file:
        timeout_file.write(','.join(all_timeout_tests))
    with open('./failed', 'w') as failed_file:
        failed_file.write(','.join(all_failed_tests))
dev_support_dir = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(dev_support_dir, 'flaky-dashboard-template.html'), 'r') as f:
    template = Template(f.read())
with open('dashboard.html', 'w') as f:
    datetime = time.strftime('%m/%d/%Y %H:%M:%S')
    f.write(template.render(datetime=datetime, bad_tests_count=len(all_bad_tests), results=url_to_bad_test_results))
