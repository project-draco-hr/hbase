{
  final Map<byte[],WriterAndPath> logWriters=new TreeMap<byte[],WriterAndPath>(Bytes.BYTES_COMPARATOR);
  List<Path> splits=null;
  int logWriterThreads=conf.getInt("hbase.regionserver.hlog.splitlog.writer.threads",3);
  int concurrentLogReads=conf.getInt("hbase.regionserver.hlog.splitlog.reader.threads",3);
  try {
    int maxSteps=Double.valueOf(Math.ceil((logfiles.length * 1.0) / concurrentLogReads)).intValue();
    for (int step=0; step < maxSteps; step++) {
      final Map<byte[],LinkedList<HLogEntry>> logEntries=new TreeMap<byte[],LinkedList<HLogEntry>>(Bytes.BYTES_COMPARATOR);
      int endIndex=step == maxSteps - 1 ? logfiles.length : step * concurrentLogReads + concurrentLogReads;
      for (int i=(step * concurrentLogReads); i < endIndex; i++) {
        long length=logfiles[i].getLen();
        if (LOG.isDebugEnabled()) {
          LOG.debug("Splitting hlog " + (i + 1) + " of "+ logfiles.length+ ": "+ logfiles[i].getPath()+ ", length="+ logfiles[i].getLen());
        }
        SequenceFile.Reader in=null;
        int count=0;
        try {
          long len=fs.getFileStatus(logfiles[i].getPath()).getLen();
          in=HLog.getReader(fs,logfiles[i].getPath(),conf);
          try {
            HLogKey key=newKey(conf);
            KeyValue val=new KeyValue();
            while (in.next(key,val)) {
              byte[] regionName=key.getRegionName();
              LinkedList<HLogEntry> queue=logEntries.get(regionName);
              if (queue == null) {
                queue=new LinkedList<HLogEntry>();
                LOG.debug("Adding queue for " + Bytes.toString(regionName));
                logEntries.put(regionName,queue);
              }
              HLogEntry hle=new HLogEntry(val,key);
              queue.push(hle);
              count++;
              key=newKey(conf);
              val=new KeyValue();
            }
            LOG.debug("Pushed=" + count + " entries from "+ logfiles[i].getPath());
          }
 catch (          IOException e) {
            LOG.debug("IOE Pushed=" + count + " entries from "+ logfiles[i].getPath());
            e=RemoteExceptionHandler.checkIOException(e);
            if (!(e instanceof EOFException)) {
              LOG.warn("Exception processing " + logfiles[i].getPath() + " -- continuing. Possible DATA LOSS!",e);
            }
          }
        }
 catch (        IOException e) {
          if (length <= 0) {
            LOG.warn("Empty hlog, continuing: " + logfiles[i] + " count="+ count,e);
            continue;
          }
          throw e;
        }
 finally {
          try {
            if (in != null) {
              in.close();
            }
          }
 catch (          IOException e) {
            LOG.warn("Close in finally threw exception -- continuing",e);
          }
          fs.delete(logfiles[i].getPath(),true);
        }
      }
      ExecutorService threadPool=Executors.newFixedThreadPool(logWriterThreads);
      for (      final byte[] key : logEntries.keySet()) {
        Thread thread=new Thread(Bytes.toString(key)){
          @Override public void run(){
            LinkedList<HLogEntry> entries=logEntries.get(key);
            LOG.debug("Thread got " + entries.size() + " to process");
            long threadTime=System.currentTimeMillis();
            try {
              int count=0;
              for (ListIterator<HLogEntry> i=entries.listIterator(entries.size()); i.hasPrevious(); ) {
                HLogEntry logEntry=i.previous();
                WriterAndPath wap=logWriters.get(key);
                if (wap == null) {
                  Path logfile=new Path(HRegion.getRegionDir(HTableDescriptor.getTableDir(rootDir,logEntry.getKey().getTablename()),HRegionInfo.encodeRegionName(key)),HREGION_OLDLOGFILE_NAME);
                  Path oldlogfile=null;
                  SequenceFile.Reader old=null;
                  if (fs.exists(logfile)) {
                    LOG.warn("Old hlog file " + logfile + " already exists. Copying existing file to new file");
                    oldlogfile=new Path(logfile.toString() + ".old");
                    fs.rename(logfile,oldlogfile);
                    old=new SequenceFile.Reader(fs,oldlogfile,conf);
                  }
                  SequenceFile.Writer w=SequenceFile.createWriter(fs,conf,logfile,getKeyClass(conf),KeyValue.class,getCompressionType(conf));
                  wap=new WriterAndPath(logfile,w);
                  logWriters.put(key,wap);
                  if (LOG.isDebugEnabled()) {
                    LOG.debug("Creating new hlog file writer for path " + logfile + " and region "+ Bytes.toString(key));
                  }
                  if (old != null) {
                    HLogKey oldkey=newKey(conf);
                    KeyValue oldval=new KeyValue();
                    for (; old.next(oldkey,oldval); count++) {
                      if (LOG.isDebugEnabled() && count > 0 && count % 10000 == 0) {
                        LOG.debug("Copied " + count + " edits");
                      }
                      w.append(oldkey,oldval);
                    }
                    old.close();
                    fs.delete(oldlogfile,true);
                  }
                }
                wap.w.append(logEntry.getKey(),logEntry.getEdit());
                count++;
              }
              if (LOG.isDebugEnabled()) {
                LOG.debug("Applied " + count + " total edits to "+ Bytes.toString(key)+ " in "+ (System.currentTimeMillis() - threadTime)+ "ms");
              }
            }
 catch (            IOException e) {
              e=RemoteExceptionHandler.checkIOException(e);
              LOG.warn("Got while writing region " + Bytes.toString(key) + " log "+ e);
              e.printStackTrace();
            }
          }
        }
;
        threadPool.execute(thread);
      }
      threadPool.shutdown();
      try {
        for (int i=0; !threadPool.awaitTermination(5,TimeUnit.SECONDS); i++) {
          LOG.debug("Waiting for hlog writers to terminate, iteration #" + i);
        }
      }
 catch (      InterruptedException ex) {
        LOG.warn("Hlog writers were interrupted, possible data loss!");
      }
    }
  }
  finally {
    splits=new ArrayList<Path>(logWriters.size());
    for (    WriterAndPath wap : logWriters.values()) {
      wap.w.close();
      LOG.debug("Closed " + wap.p);
      splits.add(wap.p);
    }
  }
  return splits;
}
