{
  final Map<byte[],WriterAndPath> logWriters=new TreeMap<byte[],WriterAndPath>(Bytes.BYTES_COMPARATOR);
  List<Path> splits=null;
  int logWriterThreads=conf.getInt("hbase.regionserver.hlog.splitlog.writer.threads",3);
  int concurrentLogReads=conf.getInt("hbase.regionserver.hlog.splitlog.reader.threads",3);
  try {
    int maxSteps=Double.valueOf(Math.ceil((logfiles.length * 1.0) / concurrentLogReads)).intValue();
    for (int step=0; step < maxSteps; step++) {
      final Map<byte[],LinkedList<HLog.Entry>> logEntries=new TreeMap<byte[],LinkedList<HLog.Entry>>(Bytes.BYTES_COMPARATOR);
      int endIndex=step == maxSteps - 1 ? logfiles.length : step * concurrentLogReads + concurrentLogReads;
      for (int i=(step * concurrentLogReads); i < endIndex; i++) {
        long length=logfiles[i].getLen();
        if (LOG.isDebugEnabled()) {
          LOG.debug("Splitting hlog " + (i + 1) + " of "+ logfiles.length+ ": "+ logfiles[i].getPath()+ ", length="+ logfiles[i].getLen());
        }
        Reader in=null;
        int count=0;
        try {
          in=HLog.getReader(fs,logfiles[i].getPath(),conf);
          try {
            HLog.Entry entry;
            while ((entry=in.next()) != null) {
              byte[] regionName=entry.getKey().getRegionName();
              LinkedList<HLog.Entry> queue=logEntries.get(regionName);
              if (queue == null) {
                queue=new LinkedList<HLog.Entry>();
                LOG.debug("Adding queue for " + Bytes.toStringBinary(regionName));
                logEntries.put(regionName,queue);
              }
              queue.push(entry);
              count++;
            }
            LOG.debug("Pushed=" + count + " entries from "+ logfiles[i].getPath());
          }
 catch (          IOException e) {
            LOG.debug("IOE Pushed=" + count + " entries from "+ logfiles[i].getPath());
            e=RemoteExceptionHandler.checkIOException(e);
            if (!(e instanceof EOFException)) {
              LOG.warn("Exception processing " + logfiles[i].getPath() + " -- continuing. Possible DATA LOSS!",e);
            }
          }
        }
 catch (        IOException e) {
          if (length <= 0) {
            LOG.warn("Empty hlog, continuing: " + logfiles[i] + " count="+ count,e);
            continue;
          }
          throw e;
        }
 finally {
          try {
            if (in != null) {
              in.close();
            }
          }
 catch (          IOException e) {
            LOG.warn("Close in finally threw exception -- continuing",e);
          }
          fs.rename(logfiles[i].getPath(),getHLogArchivePath(oldLogDir,logfiles[i].getPath()));
        }
      }
      ExecutorService threadPool=Executors.newFixedThreadPool(logWriterThreads);
      for (      final byte[] key : logEntries.keySet()) {
        Thread thread=new Thread(Bytes.toStringBinary(key)){
          @Override public void run(){
            LinkedList<HLog.Entry> entries=logEntries.get(key);
            LOG.debug("Thread got " + entries.size() + " to process");
            long threadTime=System.currentTimeMillis();
            try {
              int count=0;
              for (ListIterator<HLog.Entry> i=entries.listIterator(entries.size()); i.hasPrevious(); ) {
                HLog.Entry logEntry=i.previous();
                WriterAndPath wap=logWriters.get(key);
                if (wap == null) {
                  Path logfile=new Path(HRegion.getRegionDir(HTableDescriptor.getTableDir(rootDir,logEntry.getKey().getTablename()),HRegionInfo.encodeRegionName(key)),HREGION_OLDLOGFILE_NAME);
                  Path oldlogfile=null;
                  Reader old=null;
                  if (fs.exists(logfile)) {
                    FileStatus stat=fs.getFileStatus(logfile);
                    if (stat.getLen() <= 0) {
                      LOG.warn("Old hlog file " + logfile + " is zero "+ "length. Deleting existing file");
                      fs.delete(logfile,false);
                    }
 else {
                      LOG.warn("Old hlog file " + logfile + " already "+ "exists. Copying existing file to new file");
                      oldlogfile=new Path(logfile.toString() + ".old");
                      fs.rename(logfile,oldlogfile);
                      old=getReader(fs,oldlogfile,conf);
                    }
                  }
                  Writer w=createWriter(fs,logfile,conf);
                  wap=new WriterAndPath(logfile,w);
                  logWriters.put(key,wap);
                  if (LOG.isDebugEnabled()) {
                    LOG.debug("Creating new hlog file writer for path " + logfile + " and region "+ Bytes.toStringBinary(key));
                  }
                  if (old != null) {
                    HLog.Entry entry;
                    for (; (entry=old.next()) != null; count++) {
                      if (LOG.isDebugEnabled() && count > 0 && count % 10000 == 0) {
                        LOG.debug("Copied " + count + " edits");
                      }
                      w.append(entry);
                    }
                    old.close();
                    fs.delete(oldlogfile,true);
                  }
                }
                wap.w.append(logEntry);
                count++;
              }
              if (LOG.isDebugEnabled()) {
                LOG.debug("Applied " + count + " total edits to "+ Bytes.toStringBinary(key)+ " in "+ (System.currentTimeMillis() - threadTime)+ "ms");
              }
            }
 catch (            IOException e) {
              e=RemoteExceptionHandler.checkIOException(e);
              LOG.warn("Got while writing region " + Bytes.toStringBinary(key) + " log "+ e);
              e.printStackTrace();
            }
          }
        }
;
        threadPool.execute(thread);
      }
      threadPool.shutdown();
      try {
        for (int i=0; !threadPool.awaitTermination(5,TimeUnit.SECONDS); i++) {
          LOG.debug("Waiting for hlog writers to terminate, iteration #" + i);
        }
      }
 catch (      InterruptedException ex) {
        LOG.warn("Hlog writers were interrupted, possible data loss!");
      }
    }
  }
  finally {
    splits=new ArrayList<Path>(logWriters.size());
    for (    WriterAndPath wap : logWriters.values()) {
      wap.w.close();
      LOG.debug("Closed " + wap.p);
      splits.add(wap.p);
    }
  }
  return splits;
}
