{
  if (errorMsg != null && errorMsg.length() > 0) {
    System.err.println("ERROR: " + errorMsg);
  }
  System.err.println("Usage: Export [-D <property=value>]* <tablename> <outputdir> [<versions> " + "[<starttime> [<endtime>]] [^[regex pattern] or [Prefix] to filter]]\n");
  System.err.println("  Note: -D properties will be applied to the conf used. ");
  System.err.println("  For example: ");
  System.err.println("   -D mapreduce.output.fileoutputformat.compress=true");
  System.err.println("   -D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec");
  System.err.println("   -D mapreduce.output.fileoutputformat.compress.type=BLOCK");
  System.err.println("  Additionally, the following SCAN properties can be specified");
  System.err.println("  to control/limit what is exported..");
  System.err.println("   -D " + TableInputFormat.SCAN_COLUMN_FAMILY + "=<familyName>");
  System.err.println("   -D " + RAW_SCAN + "=true");
  System.err.println("   -D " + TableInputFormat.SCAN_ROW_START + "=<ROWSTART>");
  System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
  System.err.println("For performance consider the following properties:\n" + "   -Dhbase.client.scanner.caching=100\n" + "   -Dmapreduce.map.speculative=false\n"+ "   -Dmapreduce.reduce.speculative=false");
  System.err.println("For tables with very wide rows consider setting the batch size as below:\n" + "   -D" + EXPORT_BATCHING + "=10");
}
