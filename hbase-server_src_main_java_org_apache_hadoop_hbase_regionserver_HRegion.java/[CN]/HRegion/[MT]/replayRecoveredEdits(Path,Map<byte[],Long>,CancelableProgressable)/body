{
  String msg="Replaying edits from " + edits;
  LOG.info(msg);
  MonitoredTask status=TaskMonitor.get().createStatus(msg);
  status.setStatus("Opening logs");
  HLog.Reader reader=null;
  try {
    reader=HLog.getReader(this.fs,edits,conf);
    long currentEditSeqId=-1;
    long firstSeqIdInLog=-1;
    long skippedEdits=0;
    long editsCount=0;
    long intervalEdits=0;
    HLog.Entry entry;
    Store store=null;
    boolean reported_once=false;
    try {
      int interval=this.conf.getInt("hbase.hstore.report.interval.edits",2000);
      int period=this.conf.getInt("hbase.hstore.report.period",this.conf.getInt("hbase.master.assignment.timeoutmonitor.timeout",180000) / 2);
      long lastReport=EnvironmentEdgeManager.currentTimeMillis();
      while ((entry=reader.next()) != null) {
        HLogKey key=entry.getKey();
        WALEdit val=entry.getEdit();
        if (reporter != null) {
          intervalEdits+=val.size();
          if (intervalEdits >= interval) {
            intervalEdits=0;
            long cur=EnvironmentEdgeManager.currentTimeMillis();
            if (lastReport + period <= cur) {
              status.setStatus("Replaying edits..." + " skipped=" + skippedEdits + " edits="+ editsCount);
              if (!reporter.progress()) {
                msg="Progressable reporter failed, stopping replay";
                LOG.warn(msg);
                status.abort(msg);
                throw new IOException(msg);
              }
              reported_once=true;
              lastReport=cur;
            }
          }
        }
        if (coprocessorHost != null) {
          status.setStatus("Running pre-WAL-restore hook in coprocessors");
          if (coprocessorHost.preWALRestore(this.getRegionInfo(),key,val)) {
            continue;
          }
        }
        if (firstSeqIdInLog == -1) {
          firstSeqIdInLog=key.getLogSeqNum();
        }
        boolean flush=false;
        for (        KeyValue kv : val.getKeyValues()) {
          if (kv.matchingFamily(HLog.METAFAMILY) || !Bytes.equals(key.getEncodedRegionName(),this.regionInfo.getEncodedNameAsBytes())) {
            skippedEdits++;
            continue;
          }
          if (store == null || !kv.matchingFamily(store.getFamily().getName())) {
            store=this.stores.get(kv.getFamily());
          }
          if (store == null) {
            LOG.warn("No family for " + kv);
            skippedEdits++;
            continue;
          }
          if (key.getLogSeqNum() <= maxSeqIdInStores.get(store.getFamily().getName())) {
            skippedEdits++;
            continue;
          }
          currentEditSeqId=key.getLogSeqNum();
          flush=restoreEdit(store,kv);
          editsCount++;
        }
        if (flush)         internalFlushcache(null,currentEditSeqId,status);
        if (coprocessorHost != null) {
          coprocessorHost.postWALRestore(this.getRegionInfo(),key,val);
        }
      }
    }
 catch (    EOFException eof) {
      Path p=HLog.moveAsideBadEditsFile(fs,edits);
      msg="Encountered EOF. Most likely due to Master failure during " + "log spliting, so we have this data in another edit.  " + "Continuing, but renaming " + edits + " as "+ p;
      LOG.warn(msg,eof);
      status.abort(msg);
    }
catch (    IOException ioe) {
      if (ioe.getCause() instanceof ParseException) {
        Path p=HLog.moveAsideBadEditsFile(fs,edits);
        msg="File corruption encountered!  " + "Continuing, but renaming " + edits + " as "+ p;
        LOG.warn(msg,ioe);
        status.setStatus(msg);
      }
 else {
        status.abort(StringUtils.stringifyException(ioe));
        throw ioe;
      }
    }
    if (reporter != null && !reported_once) {
      reporter.progress();
    }
    msg="Applied " + editsCount + ", skipped "+ skippedEdits+ ", firstSequenceidInLog="+ firstSeqIdInLog+ ", maxSequenceidInLog="+ currentEditSeqId+ ", path="+ edits;
    status.markComplete(msg);
    LOG.debug(msg);
    return currentEditSeqId;
  }
  finally {
    status.cleanup();
    if (reader != null) {
      reader.close();
    }
  }
}
