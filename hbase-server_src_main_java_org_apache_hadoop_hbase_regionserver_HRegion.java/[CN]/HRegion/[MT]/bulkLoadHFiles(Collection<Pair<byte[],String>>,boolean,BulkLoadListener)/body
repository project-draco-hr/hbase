{
  long seqId=-1;
  Map<byte[],List<Path>> storeFiles=new TreeMap<byte[],List<Path>>(Bytes.BYTES_COMPARATOR);
  Map<String,Long> storeFilesSizes=new HashMap<String,Long>();
  Preconditions.checkNotNull(familyPaths);
  startBulkRegionOperation(hasMultipleColumnFamilies(familyPaths));
  boolean isSuccessful=false;
  try {
    this.writeRequestsCount.increment();
    List<IOException> ioes=new ArrayList<IOException>();
    List<Pair<byte[],String>> failures=new ArrayList<Pair<byte[],String>>();
    for (    Pair<byte[],String> p : familyPaths) {
      byte[] familyName=p.getFirst();
      String path=p.getSecond();
      Store store=getStore(familyName);
      if (store == null) {
        IOException ioe=new org.apache.hadoop.hbase.DoNotRetryIOException("No such column family " + Bytes.toStringBinary(familyName));
        ioes.add(ioe);
      }
 else {
        try {
          store.assertBulkLoadHFileOk(new Path(path));
        }
 catch (        WrongRegionException wre) {
          failures.add(p);
        }
catch (        IOException ioe) {
          ioes.add(ioe);
        }
      }
    }
    if (ioes.size() != 0) {
      IOException e=MultipleIOException.createIOException(ioes);
      LOG.error("There were one or more IO errors when checking if the bulk load is ok.",e);
      throw e;
    }
    if (failures.size() != 0) {
      StringBuilder list=new StringBuilder();
      for (      Pair<byte[],String> p : failures) {
        list.append("\n").append(Bytes.toString(p.getFirst())).append(" : ").append(p.getSecond());
      }
      LOG.warn("There was a recoverable bulk load failure likely due to a" + " split.  These (family, HFile) pairs were not loaded: " + list);
      return isSuccessful;
    }
    if (assignSeqId) {
      FlushResult fs=flushcache(true,false);
      if (fs.isFlushSucceeded()) {
        seqId=((FlushResultImpl)fs).flushSequenceId;
      }
 else       if (fs.getResult() == FlushResult.Result.CANNOT_FLUSH_MEMSTORE_EMPTY) {
        seqId=((FlushResultImpl)fs).flushSequenceId;
      }
 else {
        throw new IOException("Could not bulk load with an assigned sequential ID because the " + "flush didn't run. Reason for not flushing: " + ((FlushResultImpl)fs).failureReason);
      }
    }
    for (    Pair<byte[],String> p : familyPaths) {
      byte[] familyName=p.getFirst();
      String path=p.getSecond();
      Store store=getStore(familyName);
      try {
        String finalPath=path;
        if (bulkLoadListener != null) {
          finalPath=bulkLoadListener.prepareBulkLoad(familyName,path);
        }
        Path commitedStoreFile=store.bulkLoadHFile(finalPath,seqId);
        try {
          FileSystem fs=commitedStoreFile.getFileSystem(baseConf);
          storeFilesSizes.put(commitedStoreFile.getName(),fs.getFileStatus(commitedStoreFile).getLen());
        }
 catch (        IOException e) {
          LOG.warn("Failed to find the size of hfile " + commitedStoreFile);
          storeFilesSizes.put(commitedStoreFile.getName(),0L);
        }
        if (storeFiles.containsKey(familyName)) {
          storeFiles.get(familyName).add(commitedStoreFile);
        }
 else {
          List<Path> storeFileNames=new ArrayList<Path>();
          storeFileNames.add(commitedStoreFile);
          storeFiles.put(familyName,storeFileNames);
        }
        if (bulkLoadListener != null) {
          bulkLoadListener.doneBulkLoad(familyName,path);
        }
      }
 catch (      IOException ioe) {
        LOG.error("There was a partial failure due to IO when attempting to" + " load " + Bytes.toString(p.getFirst()) + " : "+ p.getSecond(),ioe);
        if (bulkLoadListener != null) {
          try {
            bulkLoadListener.failedBulkLoad(familyName,path);
          }
 catch (          Exception ex) {
            LOG.error("Error while calling failedBulkLoad for family " + Bytes.toString(familyName) + " with path "+ path,ex);
          }
        }
        throw ioe;
      }
    }
    isSuccessful=true;
  }
  finally {
    if (wal != null && !storeFiles.isEmpty()) {
      try {
        WALProtos.BulkLoadDescriptor loadDescriptor=ProtobufUtil.toBulkLoadDescriptor(this.getRegionInfo().getTable(),ByteStringer.wrap(this.getRegionInfo().getEncodedNameAsBytes()),storeFiles,storeFilesSizes,seqId);
        WALUtil.writeBulkLoadMarkerAndSync(this.wal,this.getReplicationScope(),getRegionInfo(),loadDescriptor,mvcc);
      }
 catch (      IOException ioe) {
        if (this.rsServices != null) {
          isSuccessful=false;
          this.rsServices.abort("Failed to write bulk load event into WAL.",ioe);
        }
      }
    }
    closeBulkRegionOperation();
  }
  return isSuccessful;
}
