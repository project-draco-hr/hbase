{
  long seqId=-1;
  Map<byte[],List<Path>> storeFiles=new TreeMap<byte[],List<Path>>(Bytes.BYTES_COMPARATOR);
  Preconditions.checkNotNull(familyPaths);
  startBulkRegionOperation(hasMultipleColumnFamilies(familyPaths));
  try {
    this.writeRequestsCount.increment();
    List<IOException> ioes=new ArrayList<IOException>();
    List<Pair<byte[],String>> failures=new ArrayList<Pair<byte[],String>>();
    for (    Pair<byte[],String> p : familyPaths) {
      byte[] familyName=p.getFirst();
      String path=p.getSecond();
      Store store=getStore(familyName);
      if (store == null) {
        IOException ioe=new org.apache.hadoop.hbase.DoNotRetryIOException("No such column family " + Bytes.toStringBinary(familyName));
        ioes.add(ioe);
      }
 else {
        try {
          store.assertBulkLoadHFileOk(new Path(path));
        }
 catch (        WrongRegionException wre) {
          failures.add(p);
        }
catch (        IOException ioe) {
          ioes.add(ioe);
        }
      }
    }
    if (ioes.size() != 0) {
      IOException e=MultipleIOException.createIOException(ioes);
      LOG.error("There were one or more IO errors when checking if the bulk load is ok.",e);
      throw e;
    }
    if (failures.size() != 0) {
      StringBuilder list=new StringBuilder();
      for (      Pair<byte[],String> p : failures) {
        list.append("\n").append(Bytes.toString(p.getFirst())).append(" : ").append(p.getSecond());
      }
      LOG.warn("There was a recoverable bulk load failure likely due to a" + " split.  These (family, HFile) pairs were not loaded: " + list);
      return false;
    }
    if (assignSeqId) {
      FlushResult fs=this.flushcache();
      if (fs.isFlushSucceeded()) {
        seqId=fs.flushSequenceId;
      }
 else       if (fs.result == FlushResult.Result.CANNOT_FLUSH_MEMSTORE_EMPTY) {
        seqId=fs.flushSequenceId;
      }
 else {
        throw new IOException("Could not bulk load with an assigned sequential ID because the " + "flush didn't run. Reason for not flushing: " + fs.failureReason);
      }
    }
    for (    Pair<byte[],String> p : familyPaths) {
      byte[] familyName=p.getFirst();
      String path=p.getSecond();
      Store store=getStore(familyName);
      try {
        String finalPath=path;
        if (bulkLoadListener != null) {
          finalPath=bulkLoadListener.prepareBulkLoad(familyName,path);
        }
        store.bulkLoadHFile(finalPath,seqId);
        if (storeFiles.containsKey(familyName)) {
          storeFiles.get(familyName).add(new Path(finalPath));
        }
 else {
          List<Path> storeFileNames=new ArrayList<Path>();
          storeFileNames.add(new Path(finalPath));
          storeFiles.put(familyName,storeFileNames);
        }
        if (bulkLoadListener != null) {
          bulkLoadListener.doneBulkLoad(familyName,path);
        }
      }
 catch (      IOException ioe) {
        LOG.error("There was a partial failure due to IO when attempting to" + " load " + Bytes.toString(p.getFirst()) + " : "+ p.getSecond(),ioe);
        if (bulkLoadListener != null) {
          try {
            bulkLoadListener.failedBulkLoad(familyName,path);
          }
 catch (          Exception ex) {
            LOG.error("Error while calling failedBulkLoad for family " + Bytes.toString(familyName) + " with path "+ path,ex);
          }
        }
        throw ioe;
      }
    }
    return true;
  }
  finally {
    if (wal != null && !storeFiles.isEmpty()) {
      try {
        WALProtos.BulkLoadDescriptor loadDescriptor=ProtobufUtil.toBulkLoadDescriptor(this.getRegionInfo().getTable(),ByteStringer.wrap(this.getRegionInfo().getEncodedNameAsBytes()),storeFiles,seqId);
        WALUtil.writeBulkLoadMarkerAndSync(wal,this.htableDescriptor,getRegionInfo(),loadDescriptor,sequenceId);
      }
 catch (      IOException ioe) {
        if (this.rsServices != null) {
          this.rsServices.abort("Failed to write bulk load event into WAL.",ioe);
        }
      }
    }
    closeBulkRegionOperation();
  }
}
