{
  TreeMap<byte[],StoreFlushContext> storeFlushCtxs=prepareResult.storeFlushCtxs;
  TreeMap<byte[],List<Path>> committedFiles=prepareResult.committedFiles;
  long startTime=prepareResult.startTime;
  long flushOpSeqId=prepareResult.flushOpSeqId;
  long flushedSeqId=prepareResult.flushedSeqId;
  long totalFlushableSizeOfFlushableStores=prepareResult.totalFlushableSize;
  String s="Flushing stores of " + this;
  status.setStatus(s);
  if (LOG.isTraceEnabled())   LOG.trace(s);
  boolean compactionRequested=false;
  long flushedOutputFileSize=0;
  try {
    for (    StoreFlushContext flush : storeFlushCtxs.values()) {
      flush.flushCache(status);
    }
    Iterator<Store> it=storesToFlush.iterator();
    for (    StoreFlushContext flush : storeFlushCtxs.values()) {
      boolean needsCompaction=flush.commit(status);
      if (needsCompaction) {
        compactionRequested=true;
      }
      byte[] storeName=it.next().getFamily().getName();
      List<Path> storeCommittedFiles=flush.getCommittedFiles();
      committedFiles.put(storeName,storeCommittedFiles);
      if (storeCommittedFiles == null || storeCommittedFiles.isEmpty()) {
        totalFlushableSizeOfFlushableStores-=prepareResult.storeFlushableSize.get(storeName);
      }
      flushedOutputFileSize+=flush.getOutputFileSize();
    }
    storeFlushCtxs.clear();
    this.addAndGetGlobalMemstoreSize(-totalFlushableSizeOfFlushableStores);
    if (wal != null) {
      FlushDescriptor desc=ProtobufUtil.toFlushDescriptor(FlushAction.COMMIT_FLUSH,getRegionInfo(),flushOpSeqId,committedFiles);
      WALUtil.writeFlushMarker(wal,this.getReplicationScope(),getRegionInfo(),desc,true,mvcc);
    }
  }
 catch (  Throwable t) {
    if (wal != null) {
      try {
        FlushDescriptor desc=ProtobufUtil.toFlushDescriptor(FlushAction.ABORT_FLUSH,getRegionInfo(),flushOpSeqId,committedFiles);
        WALUtil.writeFlushMarker(wal,this.replicationScope,getRegionInfo(),desc,false,mvcc);
      }
 catch (      Throwable ex) {
        LOG.warn(getRegionInfo().getEncodedName() + " : " + "failed writing ABORT_FLUSH marker to WAL",ex);
      }
      wal.abortCacheFlush(this.getRegionInfo().getEncodedNameAsBytes());
    }
    DroppedSnapshotException dse=new DroppedSnapshotException("region: " + Bytes.toStringBinary(getRegionInfo().getRegionName()));
    dse.initCause(t);
    status.abort("Flush failed: " + StringUtils.stringifyException(t));
    this.closing.set(true);
    if (rsServices != null) {
      rsServices.abort("Replay of WAL required. Forcing server shutdown",dse);
    }
    throw dse;
  }
  for (  Store storeToFlush : storesToFlush) {
    ((HStore)storeToFlush).finalizeFlush();
  }
  if (wal != null) {
    wal.completeCacheFlush(this.getRegionInfo().getEncodedNameAsBytes());
  }
  for (  Store store : storesToFlush) {
    this.lastStoreFlushTimeMap.put(store,startTime);
  }
  this.maxFlushedSeqId=flushedSeqId;
  this.lastFlushOpSeqId=flushOpSeqId;
synchronized (this) {
    notifyAll();
  }
  long time=EnvironmentEdgeManager.currentTime() - startTime;
  long memstoresize=this.memstoreSize.get();
  String msg="Finished memstore flush of ~" + StringUtils.byteDesc(totalFlushableSizeOfFlushableStores) + "/"+ totalFlushableSizeOfFlushableStores+ ", currentsize="+ StringUtils.byteDesc(memstoresize)+ "/"+ memstoresize+ " for region "+ this+ " in "+ time+ "ms, sequenceid="+ flushOpSeqId+ ", compaction requested="+ compactionRequested+ ((wal == null) ? "; wal=null" : "");
  LOG.info(msg);
  status.setStatus(msg);
  if (rsServices != null && rsServices.getMetrics() != null) {
    rsServices.getMetrics().updateFlush(time - startTime,totalFlushableSizeOfFlushableStores,flushedOutputFileSize);
  }
  return new FlushResultImpl(compactionRequested ? FlushResult.Result.FLUSHED_COMPACTION_NEEDED : FlushResult.Result.FLUSHED_NO_COMPACTION_NEEDED,flushOpSeqId);
}
