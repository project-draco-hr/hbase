{
  byte[] row=increment.getRow();
  checkRow(row,"increment");
  TimeRange tr=increment.getTimeRange();
  boolean flush=false;
  Durability durability=getEffectiveDurability(increment.getDurability());
  boolean writeToWAL=durability != Durability.SKIP_WAL;
  WALEdit walEdits=null;
  List<KeyValue> allKVs=new ArrayList<KeyValue>(increment.size());
  Map<Store,List<KeyValue>> tempMemstore=new HashMap<Store,List<KeyValue>>();
  long size=0;
  long txid=0;
  checkReadOnly();
  startRegionOperation(Operation.INCREMENT);
  this.writeRequestsCount.increment();
  WriteEntry w=null;
  try {
    RowLock rowLock=getRowLock(row);
    try {
      lock(this.updatesLock.readLock());
      mvcc.completeMemstoreInsert(mvcc.beginMemstoreInsert());
      w=mvcc.beginMemstoreInsert();
      try {
        long now=EnvironmentEdgeManager.currentTimeMillis();
        for (        Map.Entry<byte[],List<? extends Cell>> family : increment.getFamilyMap().entrySet()) {
          Store store=stores.get(family.getKey());
          List<KeyValue> kvs=new ArrayList<KeyValue>(family.getValue().size());
          Get get=new Get(row);
          for (          Cell cell : family.getValue()) {
            KeyValue kv=KeyValueUtil.ensureKeyValue(cell);
            get.addColumn(family.getKey(),kv.getQualifier());
          }
          get.setTimeRange(tr.getMin(),tr.getMax());
          List<KeyValue> results=get(get,false);
          int idx=0;
          for (          Cell cell : family.getValue()) {
            KeyValue kv=KeyValueUtil.ensureKeyValue(cell);
            long amount=Bytes.toLong(kv.getValue());
            byte[] qualifier=kv.getQualifier();
            if (idx < results.size() && results.get(idx).matchingQualifier(qualifier)) {
              kv=results.get(idx);
              if (kv.getValueLength() == Bytes.SIZEOF_LONG) {
                amount+=Bytes.toLong(kv.getBuffer(),kv.getValueOffset(),Bytes.SIZEOF_LONG);
              }
 else {
                throw new org.apache.hadoop.hbase.DoNotRetryIOException("Attempted to increment field that isn't 64 bits wide");
              }
              idx++;
            }
            KeyValue newKV=new KeyValue(row,family.getKey(),qualifier,now,Bytes.toBytes(amount));
            newKV.setMemstoreTS(w.getWriteNumber());
            kvs.add(newKV);
            if (writeToWAL) {
              if (walEdits == null) {
                walEdits=new WALEdit();
              }
              walEdits.add(newKV);
            }
          }
          tempMemstore.put(store,kvs);
        }
        if (writeToWAL) {
          txid=this.log.appendNoSync(this.getRegionInfo(),this.htableDescriptor.getTableName(),walEdits,HConstants.DEFAULT_CLUSTER_ID,EnvironmentEdgeManager.currentTimeMillis(),this.htableDescriptor);
        }
 else {
          recordMutationWithoutWal(increment.getFamilyMap());
        }
        for (        Map.Entry<Store,List<KeyValue>> entry : tempMemstore.entrySet()) {
          Store store=entry.getKey();
          if (store.getFamily().getMaxVersions() == 1) {
            size+=store.upsert(entry.getValue(),getSmallestReadPoint());
          }
 else {
            for (            Cell cell : entry.getValue()) {
              KeyValue kv=KeyValueUtil.ensureKeyValue(cell);
              size+=store.add(kv);
            }
          }
          allKVs.addAll(entry.getValue());
        }
        size=this.addAndGetGlobalMemstoreSize(size);
        flush=isFlushSize(size);
      }
  finally {
        this.updatesLock.readLock().unlock();
      }
    }
  finally {
      rowLock.release();
    }
    if (writeToWAL) {
      syncOrDefer(txid,durability);
    }
  }
  finally {
    if (w != null) {
      mvcc.completeMemstoreInsert(w);
    }
    closeRegionOperation();
    if (this.metricsRegion != null) {
      this.metricsRegion.updateIncrement();
    }
  }
  if (flush) {
    requestFlush();
  }
  return new Result(allKVs);
}
