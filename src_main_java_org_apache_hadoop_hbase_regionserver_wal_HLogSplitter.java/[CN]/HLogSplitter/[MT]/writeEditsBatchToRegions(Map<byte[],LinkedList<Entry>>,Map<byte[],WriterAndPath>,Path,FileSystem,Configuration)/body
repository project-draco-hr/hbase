{
  int logWriterThreads=conf.getInt("hbase.regionserver.hlog.splitlog.writer.threads",3);
  boolean skipErrors=conf.getBoolean("hbase.skip.errors",false);
  HashMap<byte[],Future> writeFutureResult=new HashMap<byte[],Future>();
  ThreadFactoryBuilder builder=new ThreadFactoryBuilder();
  builder.setNameFormat("SplitWriter-%1$d");
  ThreadFactory factory=builder.build();
  ThreadPoolExecutor threadPool=(ThreadPoolExecutor)Executors.newFixedThreadPool(logWriterThreads,factory);
  for (  final byte[] region : splitLogsMap.keySet()) {
    Callable splitter=createNewSplitter(rootDir,logWriters,splitLogsMap,region,fs,conf);
    writeFutureResult.put(region,threadPool.submit(splitter));
  }
  threadPool.shutdown();
  try {
    for (int j=0; !threadPool.awaitTermination(5,TimeUnit.SECONDS); j++) {
      String message="Waiting for hlog writers to terminate, elapsed " + j * 5 + " seconds";
      if (j < 30) {
        LOG.debug(message);
      }
 else {
        LOG.info(message);
      }
    }
  }
 catch (  InterruptedException ex) {
    LOG.warn("Hlog writers were interrupted, possible data loss!");
    if (!skipErrors) {
      throw new IOException("Could not finish writing log entries",ex);
    }
  }
  for (  Map.Entry<byte[],Future> entry : writeFutureResult.entrySet()) {
    try {
      entry.getValue().get();
    }
 catch (    ExecutionException e) {
      throw (new IOException(e.getCause()));
    }
catch (    InterruptedException e1) {
      LOG.warn("Writer for region " + Bytes.toString(entry.getKey()) + " was interrupted, however the write process should have "+ "finished. Throwing up ",e1);
      throw (new IOException(e1.getCause()));
    }
  }
}
