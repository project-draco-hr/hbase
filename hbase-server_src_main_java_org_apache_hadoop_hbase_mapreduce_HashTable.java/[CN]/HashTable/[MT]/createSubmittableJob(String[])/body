{
  Path partitionsPath=new Path(destPath,PARTITIONS_FILE_NAME);
  generatePartitions(partitionsPath);
  Job job=Job.getInstance(getConf(),getConf().get("mapreduce.job.name","hashTable_" + tableHash.tableName));
  Configuration jobConf=job.getConfiguration();
  jobConf.setLong(HASH_BATCH_SIZE_CONF_KEY,tableHash.batchSize);
  job.setJarByClass(HashTable.class);
  TableMapReduceUtil.initTableMapperJob(tableHash.tableName,tableHash.initScan(),HashMapper.class,ImmutableBytesWritable.class,ImmutableBytesWritable.class,job);
  job.setPartitionerClass(TotalOrderPartitioner.class);
  TotalOrderPartitioner.setPartitionFile(jobConf,partitionsPath);
  job.setReducerClass(Reducer.class);
  job.setNumReduceTasks(tableHash.numHashFiles);
  job.setOutputKeyClass(ImmutableBytesWritable.class);
  job.setOutputValueClass(ImmutableBytesWritable.class);
  job.setOutputFormatClass(MapFileOutputFormat.class);
  FileOutputFormat.setOutputPath(job,new Path(destPath,HASH_DATA_DIR));
  return job;
}
